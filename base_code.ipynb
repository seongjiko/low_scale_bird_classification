{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from autoaugment import ImageNetPolicy\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from torch.utils.data import Subset\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, csv, transforms, is_low = False, is_test=False, debug=False):\n",
    "        if debug:\n",
    "            csv = csv[::500]\n",
    "            \n",
    "        \n",
    "        self.is_test = is_test\n",
    "        if is_low:\n",
    "            self.path = csv['img_path'].values\n",
    "        \n",
    "        else:\n",
    "            self.path = csv['upscale_img_path'].values\n",
    "\n",
    "        if not is_test:\n",
    "            self.class_ = csv['label'].values\n",
    "\n",
    "        self.transform = transforms\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = np.array(Image.open(self.path[idx]).convert('RGB'))\n",
    "        img = Image.fromarray(img)\n",
    "        \n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        if not self.is_test:\n",
    "            y = self.class_[idx]\n",
    "\n",
    "            return img, y\n",
    "        \n",
    "        else:\n",
    "            return img\n",
    "        \n",
    "\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.path)\n",
    "\n",
    "\n",
    "def get_transforms_AutoAug(is_low):\n",
    "    size = 64 if is_low else 256\n",
    "\n",
    "    train_transforms = transforms.Compose([\n",
    "        transforms.Resize(size), \n",
    "        transforms.RandomHorizontalFlip(), \n",
    "        ImageNetPolicy(), \n",
    "        transforms.ToTensor(), \n",
    "        transforms.Normalize(mean = [0.485, 0.456, 0.406], std = [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    valid_transforms = transforms.Compose([\n",
    "        transforms.Resize(size),\n",
    "        transforms.ToTensor(), \n",
    "        transforms.Normalize(mean = [0.485, 0.456, 0.406], std = [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "    return train_transforms, valid_transforms\n",
    "\n",
    "def get_loader(cfg, is_test=False):\n",
    "    if cfg['use_kfold']:\n",
    "        df = pd.read_csv('train.csv')\n",
    "        kfold = KFold(n_splits=3, shuffle=True, random_state=1020)\n",
    "        \n",
    "        train_transforms, valid_transforms = get_transforms_AutoAug(cfg['is_low'])\n",
    "\n",
    "        # K-Fold의 각 분할에 대해 DataLoader 리스트를 초기화합니다.\n",
    "        train_loaders = []\n",
    "        valid_loaders = []\n",
    "        \n",
    "        for fold, (train_idx, valid_idx) in enumerate(kfold.split(df)):\n",
    "            # 훈련 및 검증 데이터프레임을 생성합니다.\n",
    "            train_df = df.iloc[train_idx]\n",
    "            valid_df = df.iloc[valid_idx]\n",
    "\n",
    "            encoder = LabelEncoder()\n",
    "            train_df['label'] = encoder.fit_transform(train_df['label'])\n",
    "            valid_df['label'] = encoder.transform(valid_df['label'])\n",
    "            \n",
    "            # CustomDataset을 사용하여 훈련 및 검증 데이터셋을 생성합니다.\n",
    "            train_dataset = CustomDataset(train_df, train_transforms, cfg['is_low'], False, cfg['debug'])\n",
    "            valid_dataset = CustomDataset(valid_df, valid_transforms, cfg['is_low'], False, cfg['debug'])\n",
    "            \n",
    "            # DataLoader 인스턴스를 생성합니다.\n",
    "            train_loader = DataLoader(train_dataset, batch_size=cfg['batch_size'], shuffle=True)\n",
    "            valid_loader = DataLoader(valid_dataset, batch_size=1, shuffle=False)\n",
    "            \n",
    "            train_loaders.append(train_loader)\n",
    "            valid_loaders.append(valid_loader)\n",
    "\n",
    "        return train_loaders, valid_loaders\n",
    "        \n",
    "    else:\n",
    "        if is_test:\n",
    "            test_df = pd.read_csv('test.csv')\n",
    "            test_dataset = CustomDataset(test_df, valid_transforms, cfg['is_low'], is_test=True)\n",
    "            test_loader = DataLoader(test_dataset, batch_size=cfg['batch_size'], shuffle=False)\n",
    "\n",
    "        else:\n",
    "            df = pd.read_csv('train.csv')\n",
    "            train_df, valid_df = train_test_split(df, test_size=0.2, random_state=1020, stratify=df['label'])\n",
    "\n",
    "            encoder = LabelEncoder()\n",
    "            train_df['label'] = encoder.fit_transform(train_df['label'])\n",
    "            valid_df['label'] = encoder.transform(valid_df['label'])\n",
    "\n",
    "            train_transforms, valid_transforms = get_transforms_AutoAug(cfg['is_low'])\n",
    "            train_dataset = CustomDataset(train_df, train_transforms, cfg['is_low'], False, cfg['debug'])\n",
    "            valid_dataset = CustomDataset(valid_df, valid_transforms, cfg['is_low'], False, cfg['debug'])\n",
    "\n",
    "            train_loader = DataLoader(train_dataset, batch_size=cfg['batch_size'], shuffle=True)\n",
    "            valid_loader = DataLoader(valid_dataset, batch_size=1, shuffle=False)\n",
    "    \n",
    "\n",
    "    if not is_test:\n",
    "        return train_loader, valid_loader\n",
    "    \n",
    "    else:\n",
    "        return test_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# models.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Seongji Ko pro\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import timm\n",
    "\n",
    "def get_convnext_base():\n",
    "    model = timm.create_model('convnext_base', pretrained=True, num_classes=25)\n",
    "    model = model.to('cuda:1')\n",
    "\n",
    "    return model\n",
    "\n",
    "def get_convnext_large():\n",
    "    model = timm.create_model('convnext_large', pretrained=True, num_classes=25)\n",
    "    model = model.to('cuda:1')\n",
    "\n",
    "    return model\n",
    "\n",
    "def get_mobileNetV3_large():\n",
    "    model = timm.create_model('mobilenetv3_large_100', pretrained=True, num_classes=25)\n",
    "    model = model.to('cuda:1')\n",
    "\n",
    "    return model\n",
    "\n",
    "def get_resnet50_32x4d():\n",
    "    model = timm.create_model('resnext50_32x4d', pretrained=True, num_classes=25)\n",
    "    model = model.to('cuda:1')\n",
    "\n",
    "    return model\n",
    "\n",
    "def get_swin_large():\n",
    "    model = timm.create_model('swin_large_patch4_window7_224', pretrained=True, num_classes=25)\n",
    "    model = model.to('cuda:1')\n",
    "\n",
    "    return model\n",
    "\n",
    "def get_deit3_large():\n",
    "    model = timm.create_model('deit3_large_patch16_224', pretrained=True, num_classes=25)\n",
    "    model = model.to('cuda:1')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trainer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from collections import deque\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "import numpy as np\n",
    "import csv\n",
    "import torch\n",
    "from tqdm.cli import tqdm\n",
    "\n",
    "def run_model(model, loader, loss_fn=None, optimizer=None, is_training=False, epoch=None, is_test = False):\n",
    "    targets = []\n",
    "    preds = []\n",
    "    smooth_loss_queue = deque(maxlen=100)\n",
    "\n",
    "    if is_training:\n",
    "        model.train()\n",
    "        mode = 'Train'\n",
    "    else:\n",
    "        model.eval()\n",
    "        mode = 'Valid/Test'\n",
    "\n",
    "    running_loss = 0.0\n",
    "    bar = tqdm(loader, ascii=True)\n",
    "\n",
    "    if not is_test:\n",
    "        for cnt, (data, target) in enumerate(bar):\n",
    "            data = data.to('cuda:1')\n",
    "            target = target.to('cuda:1')\n",
    "            if is_training:\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "            outputs = model(data)\n",
    "\n",
    "            if outputs.dim() == 0:\n",
    "                outputs = outputs.unsqueeze(0)\n",
    "\n",
    "            if target.dim() == 0:\n",
    "                target = target.unsqueeze(0)\n",
    "\n",
    "            if not is_test:\n",
    "                total_loss = loss_fn(outputs, target.long())\n",
    "                running_loss += total_loss.item()\n",
    "                smooth_loss_queue.append(total_loss.item())\n",
    "                smooth_loss = sum(smooth_loss_queue) / len(smooth_loss_queue)\n",
    "\n",
    "            predicted = torch.argmax(outputs, dim=-1)\n",
    "\n",
    "            if not is_test:\n",
    "                preds.extend(predicted.detach().cpu().tolist())\n",
    "                targets.extend(target.detach().cpu().tolist())\n",
    "\n",
    "            else:\n",
    "                preds.extend(predicted.detach().cpu().tolist())\n",
    "\n",
    "            if is_training:\n",
    "                total_loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            \n",
    "            bar.set_description(f'Loss: {total_loss:.6f} | Smooth Loss: {smooth_loss:.6f}')\n",
    "        f1_score_ = f1_score(np.array(targets), np.array(preds), average='macro')\n",
    "        acc_score = accuracy_score(np.array(targets), np.array(preds))\n",
    "\n",
    "        return running_loss / len(loader), acc_score, f1_score_, np.array(targets), np.array(preds)\n",
    "\n",
    "    else:\n",
    "        preds = []\n",
    "        smooth_loss_queue = deque(maxlen=50)\n",
    "\n",
    "        model.eval()\n",
    "        mode = 'Valid/Test'\n",
    "\n",
    "        bar = tqdm(loader, ascii=True)\n",
    "\n",
    "        for cnt, (data) in enumerate(bar):\n",
    "            data = data.to('cuda:1')\n",
    "\n",
    "            outputs = model(data)\n",
    "\n",
    "            if outputs.dim() == 0:\n",
    "                outputs = outputs.unsqueeze(0)\n",
    "\n",
    "            predicted = torch.argmax(outputs, dim=-1)\n",
    "            preds.extend(predicted.detach().cpu().tolist())\n",
    "            \n",
    "        return preds\n",
    "    \n",
    "\n",
    "\n",
    "def train_kfold(model, cfg, train_loaders, valid_loaders):\n",
    "    for idx, (train_loader, valid_loader) in enumerate(zip(train_loaders, valid_loaders)):\n",
    "        model = get_swin_large()\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.AdamW(model.parameters(), lr=cfg['lr'])\n",
    "        scheduler = optim.lr_scheduler.LambdaLR(optimizer=optimizer, lr_lambda=lambda epoch: 0.95 ** epoch)\n",
    "\n",
    "        best_f1 = 0\n",
    "\n",
    "        # Initialization list for train visualization\n",
    "        train_per_loss = []\n",
    "        valid_per_loss = []\n",
    "\n",
    "        train_per_acc = []\n",
    "        valid_per_acc = []\n",
    "\n",
    "        train_per_f1 = []\n",
    "        valid_per_f1 = []\n",
    "\n",
    "        # Print table header\n",
    "\n",
    "        with open(f\"logs/{cfg['attempt_name']}_{idx+1}fold.csv\", \"w\", newline='') as csvfile:\n",
    "            fieldnames = ['Epoch', 'Train Loss', 'Train Acc', 'Train F1', 'Valid Loss', 'Valid Acc', 'Valid F1', 'Learning Rate']\n",
    "            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "\n",
    "            # Write the header\n",
    "            writer.writeheader()\n",
    "            for e in range(cfg['epochs']):\n",
    "                train_loss, train_acc, train_f1, train_targets, train_preds = run_model(model, train_loader, criterion, optimizer, is_training=True, epoch=cfg['epochs'])\n",
    "                valid_loss, valid_acc, valid_f1, valid_targets, valid_preds = run_model(model, valid_loader, criterion, optimizer, is_training=False, epoch=cfg['epochs'])\n",
    "                \n",
    "                train_per_loss.append(train_loss)\n",
    "                valid_per_loss.append(valid_loss)\n",
    "                \n",
    "                train_per_acc.append(train_acc)\n",
    "                valid_per_acc.append(valid_acc)\n",
    "                \n",
    "                train_per_f1.append(train_f1)\n",
    "                valid_per_f1.append(valid_f1)\n",
    "                \n",
    "                # Print epoch results in table format\n",
    "                print(f'{\"-\"*75}')\n",
    "                print_output = f'Epoch: {e} | Train Loss: {train_loss:.6f} | Train Acc: {train_acc:.6f} | Train F1: {train_f1:.6f} | Valid Loss: {valid_loss:.6f} | Valid Acc: {valid_acc:.6f} | Valid F1: {valid_f1:.6f} | LR: {optimizer.param_groups[0][\"lr\"]:.2e}'\n",
    "                print(print_output)\n",
    "                print(f'{\"-\"*75}')\n",
    "                writer.writerow({\n",
    "                    'Epoch': e,\n",
    "                    'Train Loss': train_loss,\n",
    "                    'Train Acc': train_acc,\n",
    "                    'Train F1': train_f1,\n",
    "                    'Valid Loss': valid_loss,\n",
    "                    'Valid Acc': valid_acc,\n",
    "                    'Valid F1': valid_f1,\n",
    "                    'Learning Rate': optimizer.param_groups[0]['lr']\n",
    "                })\n",
    "                \n",
    "                scheduler.step()\n",
    "\n",
    "                if valid_f1 < best_f1:\n",
    "                    print(f'{\"*\"*75}\\nModel saved! Improved from {best_f1:.6f} to {valid_f1:.6f}\\n{\"*\"*75}')\n",
    "                    best_f1 = valid_loss\n",
    "                    torch.save(model.state_dict(), f'models/{cfg[\"attempt_name\"]}_{idx+1}fold.pt')\n",
    "                \n",
    "def train(model, cfg, train_loader, valid_loader):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=cfg['lr'])\n",
    "    scheduler = optim.lr_scheduler.LambdaLR(optimizer=optimizer, lr_lambda=lambda epoch: 0.95 ** epoch)\n",
    "\n",
    "    best_loss = float('inf')\n",
    "\n",
    "    # Initialization list for train visualization\n",
    "    train_per_loss = []\n",
    "    valid_per_loss = []\n",
    "\n",
    "    train_per_acc = []\n",
    "    valid_per_acc = []\n",
    "\n",
    "    train_per_f1 = []\n",
    "    valid_per_f1 = []\n",
    "\n",
    "    # Print table header\n",
    "\n",
    "    with open(f\"logs/{cfg['attempt_name']}.csv\", \"w\", newline='') as csvfile:\n",
    "        fieldnames = ['Epoch', 'Train Loss', 'Train Acc', 'Train F1', 'Valid Loss', 'Valid Acc', 'Valid F1', 'Learning Rate']\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "\n",
    "        # Write the header\n",
    "        writer.writeheader()\n",
    "        for e in range(cfg['epochs']):\n",
    "            train_loss, train_acc, train_f1, train_targets, train_preds = run_model(model, train_loader, criterion, optimizer, is_training=True, epoch=cfg['epochs'])\n",
    "            valid_loss, valid_acc, valid_f1, valid_targets, valid_preds = run_model(model, valid_loader, criterion, optimizer, is_training=False, epoch=cfg['epochs'])\n",
    "            \n",
    "            train_per_loss.append(train_loss)\n",
    "            valid_per_loss.append(valid_loss)\n",
    "            \n",
    "            train_per_acc.append(train_acc)\n",
    "            valid_per_acc.append(valid_acc)\n",
    "            \n",
    "            train_per_f1.append(train_f1)\n",
    "            valid_per_f1.append(valid_f1)\n",
    "            \n",
    "            # Print epoch results in table format\n",
    "            print(f'{\"-\"*75}')\n",
    "            print_output = f'Epoch: {e} | Train Loss: {train_loss:.6f} | Train Acc: {train_acc:.6f} | Train F1: {train_f1:.6f} | Valid Loss: {valid_loss:.6f} | Valid Acc: {valid_acc:.6f} | Valid F1: {valid_f1:.6f} | LR: {optimizer.param_groups[0][\"lr\"]:.2e}'\n",
    "            print(print_output)\n",
    "            print(f'{\"-\"*75}')\n",
    "            writer.writerow({\n",
    "                'Epoch': e,\n",
    "                'Train Loss': train_loss,\n",
    "                'Train Acc': train_acc,\n",
    "                'Train F1': train_f1,\n",
    "                'Valid Loss': valid_loss,\n",
    "                'Valid Acc': valid_acc,\n",
    "                'Valid F1': valid_f1,\n",
    "                'Learning Rate': optimizer.param_groups[0]['lr']\n",
    "            })\n",
    "            \n",
    "            scheduler.step()\n",
    "\n",
    "            if valid_loss < best_loss:\n",
    "                print(f'{\"*\"*75}\\nModel saved! Improved from {best_loss:.6f} to {valid_loss:.6f}\\n{\"*\"*75}')\n",
    "                best_loss = valid_loss\n",
    "                torch.save(model.state_dict(), f'models/{cfg[\"attempt_name\"]}.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random, os\n",
    "import json\n",
    "\n",
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)  # Python 내장 random 모듈\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)  # 환경변수 설정\n",
    "    np.random.seed(seed)  # NumPy\n",
    "    torch.manual_seed(seed)  # PyTorch CPU 시드 고정\n",
    "    torch.cuda.manual_seed(seed)  # PyTorch GPU 시드 고정\n",
    "    torch.cuda.manual_seed_all(seed)  # 멀티 GPU 환경에서도 시드 고정\n",
    "    torch.backends.cudnn.deterministic = True  # CuDNN 관련 설정\n",
    "    torch.backends.cudnn.benchmark = False  # 동일한 입력 크기의 데이터가 반복될 경우 속도 향상을 위한 벤치마크 모드 비활성화\n",
    "\n",
    "def train_start(cfg, seed, pretrained=False, pretrained_model_pt=''):\n",
    "    seed_everything(seed=seed)\n",
    "\n",
    "    filename = 'method_log.json'\n",
    "    # 파일이 존재하는지 확인하고, 존재하면 기존 내용을 읽습니다.\n",
    "    if os.path.exists(filename):\n",
    "        with open(filename, 'r', encoding='utf-8') as file:\n",
    "            try:\n",
    "                data = json.load(file)\n",
    "            except json.JSONDecodeError:  # 파일이 비어있거나 JSON 형식이 아닌 경우\n",
    "                data = {}\n",
    "    else:\n",
    "        data = {}\n",
    "\n",
    "    # 'attempt_name'을 키로 하여 'method' 값을 갱신하거나 추가합니다.\n",
    "    data[cfg['attempt_name']] = cfg['method']\n",
    "\n",
    "    # 변경된 데이터를 파일에 씁니다.\n",
    "    with open(filename, 'w', encoding='utf-8') as file:\n",
    "        json.dump(data, file, ensure_ascii=False, indent=4)\n",
    "\n",
    "    print(f\"'{cfg['attempt_name']}' 설정이 '{filename}' 파일에 저장되었습니다.\")\n",
    "\n",
    "    model = get_convnext_base()\n",
    "    if pretrained:\n",
    "        state_dict = torch.load(f'models/{pretrained_model_pt}.pt')\n",
    "        model.load_state_dict(state_dict)\n",
    "    print(f\"model load 완료!\")\n",
    "    \n",
    "    train_loader, valid_loader = get_loader(cfg)\n",
    "    print(f\"data load 완료!\")\n",
    "\n",
    "    print('학습을 진행합니다.')\n",
    "    train(model, cfg, train_loader, valid_loader)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# main controler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'0416_swin_large_patch16_224_KD_teacher' 설정이 'method_log.json' 파일에 저장되었습니다.\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 12\u001b[0m\n\u001b[0;32m      1\u001b[0m cfg \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m'\u001b[39m : \u001b[38;5;241m5e-5\u001b[39m,\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepochs\u001b[39m\u001b[38;5;124m'\u001b[39m : \u001b[38;5;241m6\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdebug\u001b[39m\u001b[38;5;124m'\u001b[39m : \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m     10\u001b[0m }\n\u001b[1;32m---> 12\u001b[0m \u001b[43mtrain_start\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1020\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpretrained\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[5], line 37\u001b[0m, in \u001b[0;36mtrain_start\u001b[1;34m(cfg, seed, pretrained, pretrained_model_pt)\u001b[0m\n\u001b[0;32m     33\u001b[0m     json\u001b[38;5;241m.\u001b[39mdump(data, file, ensure_ascii\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, indent\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m)\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcfg[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattempt_name\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m 설정이 \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilename\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m 파일에 저장되었습니다.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 37\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mget_convnext_base\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pretrained:\n\u001b[0;32m     39\u001b[0m     state_dict \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodels/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpretrained_model_pt\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.pt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[3], line 7\u001b[0m, in \u001b[0;36mget_convnext_base\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_convnext_base\u001b[39m():\n\u001b[0;32m      6\u001b[0m     model \u001b[38;5;241m=\u001b[39m timm\u001b[38;5;241m.\u001b[39mcreate_model(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mconvnext_base\u001b[39m\u001b[38;5;124m'\u001b[39m, pretrained\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, num_classes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m25\u001b[39m)\n\u001b[1;32m----> 7\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcuda:1\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "File \u001b[1;32mc:\\Users\\Seongji Ko pro\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1152\u001b[0m, in \u001b[0;36mModule.to\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1148\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1149\u001b[0m                     non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[0;32m   1150\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, non_blocking)\n\u001b[1;32m-> 1152\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Seongji Ko pro\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:802\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn, recurse)\u001b[0m\n\u001b[0;32m    800\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[0;32m    801\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[1;32m--> 802\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    804\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[0;32m    805\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    806\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    807\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    812\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    813\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Seongji Ko pro\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:802\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn, recurse)\u001b[0m\n\u001b[0;32m    800\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[0;32m    801\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[1;32m--> 802\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    804\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[0;32m    805\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    806\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    807\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    812\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    813\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Seongji Ko pro\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:825\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn, recurse)\u001b[0m\n\u001b[0;32m    821\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[0;32m    822\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[0;32m    823\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[0;32m    824\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m--> 825\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    826\u001b[0m should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[0;32m    827\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m should_use_set_data:\n",
      "File \u001b[1;32mc:\\Users\\Seongji Ko pro\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1150\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[1;34m(t)\u001b[0m\n\u001b[0;32m   1147\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[0;32m   1148\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1149\u001b[0m                 non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[1;32m-> 1150\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Seongji Ko pro\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\cuda\\__init__.py:293\u001b[0m, in \u001b[0;36m_lazy_init\u001b[1;34m()\u001b[0m\n\u001b[0;32m    288\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    289\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot re-initialize CUDA in forked subprocess. To use CUDA with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    290\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultiprocessing, you must use the \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspawn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m start method\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    291\u001b[0m     )\n\u001b[0;32m    292\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(torch\u001b[38;5;241m.\u001b[39m_C, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_cuda_getDeviceCount\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 293\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTorch not compiled with CUDA enabled\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    294\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _cudart \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    295\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\n\u001b[0;32m    296\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlibcudart functions unavailable. It looks like you have a broken build?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    297\u001b[0m     )\n",
      "\u001b[1;31mAssertionError\u001b[0m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "source": [
    "cfg = {\n",
    "    'lr' : 5e-5,\n",
    "    'epochs' : 6,\n",
    "    'batch_size' : 4,\n",
    "    'is_low' : False,\n",
    "    'attempt_name' : '0416_swin_large_patch16_224_KD_teacher',\n",
    "    'method' : '고해상도 학습(Teacher) --> 저해상도 학습(distillation student)',\n",
    "    'use_kfold' : False,\n",
    "    'debug' : True\n",
    "}\n",
    "\n",
    "train_start(cfg, 1020, pretrained=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
