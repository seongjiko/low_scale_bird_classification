{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/seongjiko/low_scale_bird_classification/blob/master/base_code.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2NzXoS-NncPK"
      },
      "source": [
        "# Dataset.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "HGD0wHQRncPL"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "from autoaugment import ImageNetPolicy\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import KFold\n",
        "from torch.utils.data import Subset\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, csv, transforms, is_low=False, is_test=False, use_distillation=False, debug=False):\n",
        "        if debug:\n",
        "            csv = csv[::500]  # 디버그 모드 시 간격을 늘려서 데이터 로드 감소\n",
        "\n",
        "        self.is_test = is_test\n",
        "        self.use_distillation = use_distillation\n",
        "\n",
        "        if use_distillation:\n",
        "            print('distillation mode입니다.')\n",
        "        self.transforms = transforms\n",
        "        self.path = csv['img_path'].values\n",
        "        self.path_high = csv['upscale_img_path'].values\n",
        "\n",
        "        if not is_test:\n",
        "            self.labels = csv['label'].values\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if self.use_distillation:\n",
        "            img_low = Image.open(self.path[idx]).convert('RGB')\n",
        "            img_high = Image.open(self.path_high[idx]).convert('RGB')\n",
        "\n",
        "            if self.transforms:\n",
        "                img_low = self.transforms(img_low)\n",
        "                img_high = self.transforms(img_high)\n",
        "\n",
        "            if not self.is_test:\n",
        "                label = self.labels[idx]\n",
        "                return img_low, img_high, label\n",
        "            else:\n",
        "                return img_low, img_high\n",
        "        else:\n",
        "            img = Image.open(self.path[idx]).convert('RGB')\n",
        "\n",
        "            if self.transforms:\n",
        "                img = self.transforms(img)\n",
        "\n",
        "            if not self.is_test:\n",
        "                label = self.labels[idx]\n",
        "                return img, label\n",
        "            else:\n",
        "                return img\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.path)\n",
        "\n",
        "def get_transforms_AutoAug(is_low):\n",
        "    size = 64 if is_low else 256\n",
        "    train_transforms = transforms.Compose([\n",
        "        transforms.Resize(size),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        ImageNetPolicy(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "    ])\n",
        "    valid_transforms = transforms.Compose([\n",
        "        transforms.Resize(size),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "    ])\n",
        "    return train_transforms, valid_transforms\n",
        "\n",
        "def get_loader(cfg, is_test=False):\n",
        "    if cfg['use_kfold']:\n",
        "        df = pd.read_csv('train.csv')\n",
        "        kfold = KFold(n_splits=3, shuffle=True, random_state=1020)\n",
        "\n",
        "        train_transforms, valid_transforms = get_transforms_AutoAug(cfg['is_low'])\n",
        "\n",
        "        # K-Fold의 각 분할에 대해 DataLoader 리스트를 초기화합니다.\n",
        "        train_loaders = []\n",
        "        valid_loaders = []\n",
        "\n",
        "        for fold, (train_idx, valid_idx) in enumerate(kfold.split(df)):\n",
        "            # 훈련 및 검증 데이터프레임을 생성합니다.\n",
        "            train_df = df.iloc[train_idx]\n",
        "            valid_df = df.iloc[valid_idx]\n",
        "\n",
        "            encoder = LabelEncoder()\n",
        "            train_df['label'] = encoder.fit_transform(train_df['label'])\n",
        "            valid_df['label'] = encoder.transform(valid_df['label'])\n",
        "\n",
        "            # CustomDataset을 사용하여 훈련 및 검증 데이터셋을 생성합니다.\n",
        "            train_dataset = CustomDataset(train_df, train_transforms, cfg['is_low'], is_test=False, use_distillation=cfg['use_distillation'], debug=cfg['debug'])\n",
        "            train_dataset = CustomDataset(train_df, train_transforms, cfg['is_low'], is_test=False, use_distillation=cfg['use_distillation'], debug=cfg['debug'])\n",
        "\n",
        "            # DataLoader 인스턴스를 생성합니다.\n",
        "            train_loader = DataLoader(train_dataset, batch_size=cfg['batch_size'], shuffle=True)\n",
        "            valid_loader = DataLoader(valid_dataset, batch_size=1, shuffle=False)\n",
        "\n",
        "            train_loaders.append(train_loader)\n",
        "            valid_loaders.append(valid_loader)\n",
        "\n",
        "        return train_loaders, valid_loaders\n",
        "\n",
        "    else:\n",
        "        if is_test:\n",
        "            test_df = pd.read_csv('test.csv')\n",
        "            test_dataset = CustomDataset(test_df, valid_transforms, cfg['is_low'], is_test=True)\n",
        "            test_loader = DataLoader(test_dataset, batch_size=cfg['batch_size'], shuffle=False)\n",
        "\n",
        "        else:\n",
        "            df = pd.read_csv('train.csv')\n",
        "            train_df, valid_df = train_test_split(df, test_size=0.2, random_state=1020, stratify=df['label'])\n",
        "\n",
        "            encoder = LabelEncoder()\n",
        "            train_df['label'] = encoder.fit_transform(train_df['label'])\n",
        "            valid_df['label'] = encoder.transform(valid_df['label'])\n",
        "\n",
        "            train_transforms, valid_transforms = get_transforms_AutoAug(cfg['is_low'])\n",
        "            train_dataset = CustomDataset(train_df, train_transforms, cfg['is_low'], False, cfg['use_distillation'], cfg['debug'])\n",
        "            valid_dataset = CustomDataset(valid_df, valid_transforms, cfg['is_low'], False, cfg['use_distillation'], cfg['debug'])\n",
        "\n",
        "            train_loader = DataLoader(train_dataset, batch_size=cfg['batch_size'], shuffle=True)\n",
        "            valid_loader = DataLoader(valid_dataset, batch_size=1, shuffle=False)\n",
        "\n",
        "\n",
        "    if not is_test:\n",
        "        return train_loader, valid_loader\n",
        "\n",
        "    else:\n",
        "        return test_loader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-xPloPC3ncPM"
      },
      "source": [
        "# models.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "_IPLhYOhncPN"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import timm\n",
        "\n",
        "def get_convnext_base(pretrained=True):\n",
        "    model = timm.create_model('convnext_base', pretrained=pretrained, num_classes=25)\n",
        "    model = model.to('cuda')\n",
        "\n",
        "    return model\n",
        "\n",
        "def get_convnext_large(pretrained=True):\n",
        "    model = timm.create_model('convnext_large', pretrained=pretrained, num_classes=25)\n",
        "    model = model.to('cuda')\n",
        "\n",
        "    return model\n",
        "\n",
        "def get_mobileNetV3_large():\n",
        "    model = timm.create_model('mobilenetv3_large_100', pretrained=True, num_classes=25)\n",
        "    model = model.to('cuda')\n",
        "\n",
        "    return model\n",
        "\n",
        "def get_resnet50_32x4d():\n",
        "    model = timm.create_model('resnext50_32x4d', pretrained=True, num_classes=25)\n",
        "    model = model.to('cuda')\n",
        "\n",
        "    return model\n",
        "\n",
        "def get_swin_large(pretrained=True):\n",
        "    model = timm.create_model('swin_large_patch4_window7_224', pretrained=pretrained, num_classes=25)\n",
        "    model = model.to('cuda')\n",
        "\n",
        "    return model\n",
        "\n",
        "def get_deit3_large():\n",
        "    model = timm.create_model('deit3_large_patch16_224', pretrained=True, num_classes=25)\n",
        "    model = model.to('cuda')\n",
        "\n",
        "    return model\n",
        "\n",
        "def get_swinv2_large(pretrained=True):\n",
        "    model = timm.create_model('swinv2_large_window12to16_192to256', pretrained=pretrained, num_classes=25)\n",
        "    model = model.to('cuda')\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d1-FpAvAncPN"
      },
      "source": [
        "# Trainer.py"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def distllation(student_outputs, targets, teacher_outputs, T=20, alpha=0.75):\n",
        "    student_loss = F.cross_entropy(student_outputs, targets)\n",
        "\n",
        "    student_smoothing_softmax = F.log_softmax(student_outputs/T, dim=1)\n",
        "    teacher_smoothing_softmax = F.softmax(teacher_outputs/T, dim=1)\n",
        "    distillation_loss = nn.KLDivLoss(reduction='batchmean')(student_smoothing_softmax, teacher_smoothing_softmax)\n",
        "\n",
        "    total_loss = alpha * student_loss + (T * T)*(1-alpha) * distillation_loss\n",
        "\n",
        "    return total_loss"
      ],
      "metadata": {
        "id": "wBI2LxKgMrg_"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "h05Cx_VSMdSy"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "JeZZG7HoncPN"
      },
      "outputs": [],
      "source": [
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "from collections import deque\n",
        "from sklearn.metrics import f1_score, accuracy_score\n",
        "import numpy as np\n",
        "import csv\n",
        "import torch\n",
        "from tqdm.cli import tqdm\n",
        "\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "from sklearn.metrics import f1_score, accuracy_score\n",
        "import numpy as np\n",
        "\n",
        "def run_model(model, loader, criterion, optimizer=None, is_training=False, is_test=False, teacher_model=None):\n",
        "    model.train() if is_training else model.eval()\n",
        "    targets, preds = [], []\n",
        "    running_loss, smooth_loss = 0.0, 0.0\n",
        "    smooth_loss_queue = deque(maxlen=100)\n",
        "\n",
        "    bar = tqdm(loader, ascii=True)\n",
        "    for data, *temp in bar:\n",
        "        if len(temp) == 2: # high img 사용\n",
        "            high_data = temp[0]\n",
        "            high_data = high_data.to('cuda')\n",
        "            target = temp[1]\n",
        "            target = target.to('cuda')\n",
        "        elif len(temp) == 1:\n",
        "            target = temp[0]\n",
        "\n",
        "        data = data.to('cuda')\n",
        "        outputs = model(data)\n",
        "\n",
        "        if teacher_model:\n",
        "            teacher_model.eval()\n",
        "            with torch.no_grad():\n",
        "                teacher_outputs = teacher_model(high_data).detach()\n",
        "\n",
        "        if not is_test:\n",
        "            target = target.to('cuda')\n",
        "            if teacher_model:\n",
        "                # Using the outputs from the teacher as targets for the student\n",
        "                loss = distllation(outputs, target, teacher_outputs)  # Modify loss function as required\n",
        "            else:\n",
        "                loss = criterion(outputs, target)\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            smooth_loss_queue.append(loss.item())\n",
        "            smooth_loss = sum(smooth_loss_queue) / len(smooth_loss_queue)\n",
        "\n",
        "            bar.set_description(f'Loss: {loss.item():.6f} | Smooth Loss: {smooth_loss:.6f}')\n",
        "\n",
        "        if is_training:\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        predicted = torch.argmax(outputs, dim=-1).detach().cpu().tolist()\n",
        "        preds.extend(predicted)\n",
        "        if not is_test:\n",
        "            targets.extend(target.detach().cpu().tolist())\n",
        "\n",
        "    if not is_test:\n",
        "        f1_score_ = f1_score(np.array(targets), np.array(preds), average='macro')\n",
        "        acc_score = accuracy_score(np.array(targets), np.array(preds))\n",
        "        return running_loss / len(loader), acc_score, f1_score_, np.array(targets), np.array(preds)\n",
        "    else:\n",
        "        return preds\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train_kfold(model, cfg, train_loaders, valid_loaders):\n",
        "    for idx, (train_loader, valid_loader) in enumerate(zip(train_loaders, valid_loaders)):\n",
        "        model = get_swin_large()\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "        optimizer = optim.AdamW(model.parameters(), lr=cfg['lr'])\n",
        "        scheduler = optim.lr_scheduler.LambdaLR(optimizer=optimizer, lr_lambda=lambda epoch: 0.95 ** epoch)\n",
        "\n",
        "        best_f1 = 0\n",
        "\n",
        "        # Initialization list for train visualization\n",
        "        train_per_loss = []\n",
        "        valid_per_loss = []\n",
        "\n",
        "        train_per_acc = []\n",
        "        valid_per_acc = []\n",
        "\n",
        "        train_per_f1 = []\n",
        "        valid_per_f1 = []\n",
        "\n",
        "        # Print table header\n",
        "\n",
        "        with open(f\"logs/{cfg['attempt_name']}_{idx+1}fold.csv\", \"w\", newline='') as csvfile:\n",
        "            fieldnames = ['Epoch', 'Train Loss', 'Train Acc', 'Train F1', 'Valid Loss', 'Valid Acc', 'Valid F1', 'Learning Rate']\n",
        "            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
        "\n",
        "            # Write the header\n",
        "            writer.writeheader()\n",
        "            for e in range(cfg['epochs']):\n",
        "                train_loss, train_acc, train_f1, train_targets, train_preds = run_model(model, train_loader, criterion, optimizer, is_training=True)\n",
        "                valid_loss, valid_acc, valid_f1, valid_targets, valid_preds = run_model(model, valid_loader, criterion, optimizer, is_training=False)\n",
        "\n",
        "                train_per_loss.append(train_loss)\n",
        "                valid_per_loss.append(valid_loss)\n",
        "\n",
        "                train_per_acc.append(train_acc)\n",
        "                valid_per_acc.append(valid_acc)\n",
        "\n",
        "                train_per_f1.append(train_f1)\n",
        "                valid_per_f1.append(valid_f1)\n",
        "\n",
        "                # Print epoch results in table format\n",
        "                print(f'{\"-\"*75}')\n",
        "                print_output = f'Epoch: {e} | Train Loss: {train_loss:.6f} | Train Acc: {train_acc:.6f} | Train F1: {train_f1:.6f} | Valid Loss: {valid_loss:.6f} | Valid Acc: {valid_acc:.6f} | Valid F1: {valid_f1:.6f} | LR: {optimizer.param_groups[0][\"lr\"]:.2e}'\n",
        "                print(print_output)\n",
        "                print(f'{\"-\"*75}')\n",
        "                writer.writerow({\n",
        "                    'Epoch': e,\n",
        "                    'Train Loss': train_loss,\n",
        "                    'Train Acc': train_acc,\n",
        "                    'Train F1': train_f1,\n",
        "                    'Valid Loss': valid_loss,\n",
        "                    'Valid Acc': valid_acc,\n",
        "                    'Valid F1': valid_f1,\n",
        "                    'Learning Rate': optimizer.param_groups[0]['lr']\n",
        "                })\n",
        "\n",
        "                scheduler.step()\n",
        "\n",
        "                if valid_f1 < best_f1:\n",
        "                    print(f'{\"*\"*75}\\nModel saved! Improved from {best_f1:.6f} to {valid_f1:.6f}\\n{\"*\"*75}')\n",
        "                    best_f1 = valid_loss\n",
        "                    torch.save(model.state_dict(), f'models/{cfg[\"attempt_name\"]}_{idx+1}fold.pt')\n",
        "\n",
        "def train(model, cfg, train_loader, valid_loader):\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=cfg['lr'])\n",
        "    scheduler = optim.lr_scheduler.LambdaLR(optimizer=optimizer, lr_lambda=lambda epoch: 0.95 ** epoch)\n",
        "\n",
        "    best_f1 = 0\n",
        "\n",
        "    # Initialization list for train visualization\n",
        "    train_per_loss = []\n",
        "    valid_per_loss = []\n",
        "\n",
        "    train_per_acc = []\n",
        "    valid_per_acc = []\n",
        "\n",
        "    train_per_f1 = []\n",
        "    valid_per_f1 = []\n",
        "\n",
        "    # Print table header\n",
        "\n",
        "    with open(f\"/content/drive/MyDrive/dacon/bird_classification/logs/{cfg['attempt_name']}.csv\", \"w\", newline='') as csvfile:\n",
        "        fieldnames = ['Epoch', 'Train Loss', 'Train Acc', 'Train F1', 'Valid Loss', 'Valid Acc', 'Valid F1', 'Learning Rate']\n",
        "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
        "\n",
        "        # Write the header\n",
        "        writer.writeheader()\n",
        "        for e in range(cfg['epochs']):\n",
        "            if cfg['use_distillation']:\n",
        "                teacher_model = get_convnext_base(False)\n",
        "                teacher_model.load_state_dict(torch.load(f'/content/drive/MyDrive/dacon/bird_classification/models/0416_swin_large_patch16_224_KD_teacher.pt'))\n",
        "                teacher_model.eval()\n",
        "            else:\n",
        "                teacher_model = None\n",
        "            train_loss, train_acc, train_f1, train_targets, train_preds = run_model(model, train_loader, criterion, optimizer, is_training=True, teacher_model=teacher_model)\n",
        "            valid_loss, valid_acc, valid_f1, valid_targets, valid_preds = run_model(model, valid_loader, criterion, optimizer, is_training=False, teacher_model=teacher_model)\n",
        "\n",
        "            train_per_loss.append(train_loss)\n",
        "            valid_per_loss.append(valid_loss)\n",
        "\n",
        "            train_per_acc.append(train_acc)\n",
        "            valid_per_acc.append(valid_acc)\n",
        "\n",
        "            train_per_f1.append(train_f1)\n",
        "            valid_per_f1.append(valid_f1)\n",
        "\n",
        "            # Print epoch results in table format\n",
        "            print(f'{\"-\"*75}')\n",
        "            print_output = f'Epoch: {e} | Train Loss: {train_loss:.6f} | Train Acc: {train_acc:.6f} | Train F1: {train_f1:.6f} | Valid Loss: {valid_loss:.6f} | Valid Acc: {valid_acc:.6f} | Valid F1: {valid_f1:.6f} | LR: {optimizer.param_groups[0][\"lr\"]:.2e}'\n",
        "            print(print_output)\n",
        "            print(f'{\"-\"*75}')\n",
        "            writer.writerow({\n",
        "                'Epoch': e,\n",
        "                'Train Loss': train_loss,\n",
        "                'Train Acc': train_acc,\n",
        "                'Train F1': train_f1,\n",
        "                'Valid Loss': valid_loss,\n",
        "                'Valid Acc': valid_acc,\n",
        "                'Valid F1': valid_f1,\n",
        "                'Learning Rate': optimizer.param_groups[0]['lr']\n",
        "            })\n",
        "\n",
        "            scheduler.step()\n",
        "\n",
        "            if valid_f1 > best_f1:\n",
        "                print(f'{\"*\"*75}\\nModel saved! Improved from {best_f1:.6f} to {valid_f1:.6f}\\n{\"*\"*75}')\n",
        "                best_f1 = valid_f1\n",
        "                torch.save(model.state_dict(), f'/content/drive/MyDrive/dacon/bird_classification/models/{cfg[\"attempt_name\"]}.pt')"
      ],
      "metadata": {
        "id": "m7dP9rgAKDfw"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7yzTtFjxncPO"
      },
      "source": [
        "# Train.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "EZ1-VeWPncPO"
      },
      "outputs": [],
      "source": [
        "import random, os\n",
        "import json\n",
        "\n",
        "def seed_everything(seed=42):\n",
        "    random.seed(seed)  # Python 내장 random 모듈\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)  # 환경변수 설정\n",
        "    np.random.seed(seed)  # NumPy\n",
        "    torch.manual_seed(seed)  # PyTorch CPU 시드 고정\n",
        "    torch.cuda.manual_seed(seed)  # PyTorch GPU 시드 고정\n",
        "    torch.cuda.manual_seed_all(seed)  # 멀티 GPU 환경에서도 시드 고정\n",
        "    torch.backends.cudnn.deterministic = True  # CuDNN 관련 설정\n",
        "    torch.backends.cudnn.benchmark = False  # 동일한 입력 크기의 데이터가 반복될 경우 속도 향상을 위한 벤치마크 모드 비활성화\n",
        "\n",
        "def train_start(cfg, seed, pretrained=False, pretrained_model_pt=None):\n",
        "    seed_everything(seed=seed)\n",
        "\n",
        "    filename = 'method_log.json'\n",
        "    # 파일이 존재하는지 확인하고, 존재하면 기존 내용을 읽습니다.\n",
        "    if os.path.exists(filename):\n",
        "        with open(filename, 'r', encoding='utf-8') as file:\n",
        "            try:\n",
        "                data = json.load(file)\n",
        "            except json.JSONDecodeError:  # 파일이 비어있거나 JSON 형식이 아닌 경우\n",
        "                data = {}\n",
        "    else:\n",
        "        data = {}\n",
        "\n",
        "    # 'attempt_name'을 키로 하여 'method' 값을 갱신하거나 추가합니다.\n",
        "    data[cfg['attempt_name']] = cfg['method']\n",
        "\n",
        "    # 변경된 데이터를 파일에 씁니다.\n",
        "    with open(filename, 'w', encoding='utf-8') as file:\n",
        "        json.dump(data, file, ensure_ascii=False, indent=4)\n",
        "\n",
        "    print(f\"'{cfg['attempt_name']}' 설정이 '{filename}' 파일에 저장되었습니다.\")\n",
        "\n",
        "    model = get_swinv2_large()\n",
        "    if pretrained:\n",
        "        state_dict = torch.load(f'models/{pretrained_model_pt}.pt')\n",
        "        model.load_state_dict(state_dict)\n",
        "    print(f\"model load 완료!\")\n",
        "\n",
        "    train_loader, valid_loader = get_loader(cfg)\n",
        "    print(f\"data load 완료!\")\n",
        "\n",
        "    print('학습을 진행합니다.')\n",
        "    train(model, cfg, train_loader, valid_loader)\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wZNKh6wincPQ"
      },
      "source": [
        "# main controler"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "timm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CEyPSrfoYde4",
        "outputId": "6c08a4ef-5bd2-47c1-fc71-6f41644accb1"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<module 'timm' from '/usr/local/lib/python3.10/dist-packages/timm/__init__.py'>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "6181dbc288654304a0886788e383821c",
            "993aaec9675d4bbfb1f836ee181f327c",
            "8266fed6263241c2809a36988f1d1fb2",
            "bcebbb006e4b41d49ebe072e191436ea",
            "52aaccf97d9b42c4a9459203af40e7e8",
            "5f38eab31c61467db314c40e65f54443",
            "68aadd0cb5354d818048a580351c3290",
            "a6a57bd386f74d8e801988787894aca2",
            "84b71f4b35b4477199e3dc6a55be1f2f",
            "6edbbce39fe245ce8e15cc7b0010e547",
            "2a1ee68af53f4e348daa93c6d1605582"
          ]
        },
        "id": "3-MahjyMncPQ",
        "outputId": "40c978fb-76a2-4e23-bab0-be98cf3e7fce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'0416_swinv2_large_KD_student' 설정이 'method_log.json' 파일에 저장되었습니다.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/792M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6181dbc288654304a0886788e383821c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model load 완료!\n",
            "distillation mode입니다.\n",
            "distillation mode입니다.\n",
            "data load 완료!\n",
            "학습을 진행합니다.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss: 1.239888 | Smooth Loss: 0.558440: 100%|##########| 1584/1584 [30:30<00:00,  1.16s/it]\n",
            "Loss: 0.095115 | Smooth Loss: 0.275232: 100%|##########| 3167/3167 [05:04<00:00, 10.41it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---------------------------------------------------------------------------\n",
            "Epoch: 0 | Train Loss: 0.662622 | Train Acc: 0.870135 | Train F1: 0.869992 | Valid Loss: 0.323114 | Valid Acc: 0.940954 | Valid F1: 0.941088 | LR: 5.00e-05\n",
            "---------------------------------------------------------------------------\n",
            "***************************************************************************\n",
            "Model saved! Improved from 0.000000 to 0.941088\n",
            "***************************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss: 0.581542 | Smooth Loss: 0.384210: 100%|##########| 1584/1584 [30:27<00:00,  1.15s/it]\n",
            "Loss: 0.875381 | Smooth Loss: 0.296321: 100%|##########| 3167/3167 [05:15<00:00, 10.04it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---------------------------------------------------------------------------\n",
            "Epoch: 1 | Train Loss: 0.374875 | Train Acc: 0.929818 | Train F1: 0.929854 | Valid Loss: 0.317228 | Valid Acc: 0.937796 | Valid F1: 0.937790 | LR: 4.75e-05\n",
            "---------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss: 0.088764 | Smooth Loss: 0.261237: 100%|##########| 1584/1584 [30:40<00:00,  1.16s/it]\n",
            "Loss: 0.080815 | Smooth Loss: 0.492801: 100%|##########| 3167/3167 [05:15<00:00, 10.04it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---------------------------------------------------------------------------\n",
            "Epoch: 2 | Train Loss: 0.279654 | Train Acc: 0.946949 | Train F1: 0.946862 | Valid Loss: 0.319728 | Valid Acc: 0.936217 | Valid F1: 0.935762 | LR: 4.51e-05\n",
            "---------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss: 1.456183 | Smooth Loss: 0.268793: 100%|##########| 1584/1584 [30:36<00:00,  1.16s/it]\n",
            "Loss: 0.056289 | Smooth Loss: 0.226380: 100%|##########| 3167/3167 [05:10<00:00, 10.19it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---------------------------------------------------------------------------\n",
            "Epoch: 3 | Train Loss: 0.245727 | Train Acc: 0.954922 | Train F1: 0.954856 | Valid Loss: 0.344625 | Valid Acc: 0.935586 | Valid F1: 0.936621 | LR: 4.29e-05\n",
            "---------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss: 0.044560 | Smooth Loss: 0.217698: 100%|##########| 1584/1584 [30:38<00:00,  1.16s/it]\n",
            "Loss: 0.037028 | Smooth Loss: 0.326828: 100%|##########| 3167/3167 [05:14<00:00, 10.08it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---------------------------------------------------------------------------\n",
            "Epoch: 4 | Train Loss: 0.213753 | Train Acc: 0.962027 | Train F1: 0.962015 | Valid Loss: 0.292580 | Valid Acc: 0.941901 | Valid F1: 0.941919 | LR: 4.07e-05\n",
            "---------------------------------------------------------------------------\n",
            "***************************************************************************\n",
            "Model saved! Improved from 0.941088 to 0.941919\n",
            "***************************************************************************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loss: 0.116297 | Smooth Loss: 0.215963: 100%|##########| 1584/1584 [30:37<00:00,  1.16s/it]\n",
            "Loss: 0.055546 | Smooth Loss: 0.174841: 100%|##########| 3167/3167 [05:11<00:00, 10.16it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---------------------------------------------------------------------------\n",
            "Epoch: 5 | Train Loss: 0.190716 | Train Acc: 0.967948 | Train F1: 0.967976 | Valid Loss: 0.251584 | Valid Acc: 0.951374 | Valid F1: 0.950875 | LR: 3.87e-05\n",
            "---------------------------------------------------------------------------\n",
            "***************************************************************************\n",
            "Model saved! Improved from 0.941919 to 0.950875\n",
            "***************************************************************************\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SwinTransformerV2(\n",
              "  (patch_embed): PatchEmbed(\n",
              "    (proj): Conv2d(3, 192, kernel_size=(4, 4), stride=(4, 4))\n",
              "    (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (layers): Sequential(\n",
              "    (0): SwinTransformerV2Stage(\n",
              "      (downsample): Identity()\n",
              "      (blocks): ModuleList(\n",
              "        (0): SwinTransformerV2Block(\n",
              "          (attn): WindowAttention(\n",
              "            (cpb_mlp): Sequential(\n",
              "              (0): Linear(in_features=2, out_features=512, bias=True)\n",
              "              (1): ReLU(inplace=True)\n",
              "              (2): Linear(in_features=512, out_features=6, bias=False)\n",
              "            )\n",
              "            (qkv): Linear(in_features=192, out_features=576, bias=False)\n",
              "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "            (proj): Linear(in_features=192, out_features=192, bias=True)\n",
              "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "            (softmax): Softmax(dim=-1)\n",
              "          )\n",
              "          (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
              "          (drop_path1): Identity()\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
              "            (act): GELU(approximate='none')\n",
              "            (drop1): Dropout(p=0.0, inplace=False)\n",
              "            (norm): Identity()\n",
              "            (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
              "            (drop2): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
              "          (drop_path2): Identity()\n",
              "        )\n",
              "        (1): SwinTransformerV2Block(\n",
              "          (attn): WindowAttention(\n",
              "            (cpb_mlp): Sequential(\n",
              "              (0): Linear(in_features=2, out_features=512, bias=True)\n",
              "              (1): ReLU(inplace=True)\n",
              "              (2): Linear(in_features=512, out_features=6, bias=False)\n",
              "            )\n",
              "            (qkv): Linear(in_features=192, out_features=576, bias=False)\n",
              "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "            (proj): Linear(in_features=192, out_features=192, bias=True)\n",
              "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "            (softmax): Softmax(dim=-1)\n",
              "          )\n",
              "          (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
              "          (drop_path1): DropPath(drop_prob=0.004)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
              "            (act): GELU(approximate='none')\n",
              "            (drop1): Dropout(p=0.0, inplace=False)\n",
              "            (norm): Identity()\n",
              "            (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
              "            (drop2): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
              "          (drop_path2): DropPath(drop_prob=0.004)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (1): SwinTransformerV2Stage(\n",
              "      (downsample): PatchMerging(\n",
              "        (reduction): Linear(in_features=768, out_features=384, bias=False)\n",
              "        (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
              "      )\n",
              "      (blocks): ModuleList(\n",
              "        (0): SwinTransformerV2Block(\n",
              "          (attn): WindowAttention(\n",
              "            (cpb_mlp): Sequential(\n",
              "              (0): Linear(in_features=2, out_features=512, bias=True)\n",
              "              (1): ReLU(inplace=True)\n",
              "              (2): Linear(in_features=512, out_features=12, bias=False)\n",
              "            )\n",
              "            (qkv): Linear(in_features=384, out_features=1152, bias=False)\n",
              "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
              "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "            (softmax): Softmax(dim=-1)\n",
              "          )\n",
              "          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
              "          (drop_path1): DropPath(drop_prob=0.009)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
              "            (act): GELU(approximate='none')\n",
              "            (drop1): Dropout(p=0.0, inplace=False)\n",
              "            (norm): Identity()\n",
              "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
              "            (drop2): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
              "          (drop_path2): DropPath(drop_prob=0.009)\n",
              "        )\n",
              "        (1): SwinTransformerV2Block(\n",
              "          (attn): WindowAttention(\n",
              "            (cpb_mlp): Sequential(\n",
              "              (0): Linear(in_features=2, out_features=512, bias=True)\n",
              "              (1): ReLU(inplace=True)\n",
              "              (2): Linear(in_features=512, out_features=12, bias=False)\n",
              "            )\n",
              "            (qkv): Linear(in_features=384, out_features=1152, bias=False)\n",
              "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
              "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "            (softmax): Softmax(dim=-1)\n",
              "          )\n",
              "          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
              "          (drop_path1): DropPath(drop_prob=0.013)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
              "            (act): GELU(approximate='none')\n",
              "            (drop1): Dropout(p=0.0, inplace=False)\n",
              "            (norm): Identity()\n",
              "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
              "            (drop2): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
              "          (drop_path2): DropPath(drop_prob=0.013)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (2): SwinTransformerV2Stage(\n",
              "      (downsample): PatchMerging(\n",
              "        (reduction): Linear(in_features=1536, out_features=768, bias=False)\n",
              "        (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "      )\n",
              "      (blocks): ModuleList(\n",
              "        (0): SwinTransformerV2Block(\n",
              "          (attn): WindowAttention(\n",
              "            (cpb_mlp): Sequential(\n",
              "              (0): Linear(in_features=2, out_features=512, bias=True)\n",
              "              (1): ReLU(inplace=True)\n",
              "              (2): Linear(in_features=512, out_features=24, bias=False)\n",
              "            )\n",
              "            (qkv): Linear(in_features=768, out_features=2304, bias=False)\n",
              "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "            (softmax): Softmax(dim=-1)\n",
              "          )\n",
              "          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (drop_path1): DropPath(drop_prob=0.017)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (act): GELU(approximate='none')\n",
              "            (drop1): Dropout(p=0.0, inplace=False)\n",
              "            (norm): Identity()\n",
              "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (drop2): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (drop_path2): DropPath(drop_prob=0.017)\n",
              "        )\n",
              "        (1): SwinTransformerV2Block(\n",
              "          (attn): WindowAttention(\n",
              "            (cpb_mlp): Sequential(\n",
              "              (0): Linear(in_features=2, out_features=512, bias=True)\n",
              "              (1): ReLU(inplace=True)\n",
              "              (2): Linear(in_features=512, out_features=24, bias=False)\n",
              "            )\n",
              "            (qkv): Linear(in_features=768, out_features=2304, bias=False)\n",
              "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "            (softmax): Softmax(dim=-1)\n",
              "          )\n",
              "          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (drop_path1): DropPath(drop_prob=0.022)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (act): GELU(approximate='none')\n",
              "            (drop1): Dropout(p=0.0, inplace=False)\n",
              "            (norm): Identity()\n",
              "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (drop2): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (drop_path2): DropPath(drop_prob=0.022)\n",
              "        )\n",
              "        (2): SwinTransformerV2Block(\n",
              "          (attn): WindowAttention(\n",
              "            (cpb_mlp): Sequential(\n",
              "              (0): Linear(in_features=2, out_features=512, bias=True)\n",
              "              (1): ReLU(inplace=True)\n",
              "              (2): Linear(in_features=512, out_features=24, bias=False)\n",
              "            )\n",
              "            (qkv): Linear(in_features=768, out_features=2304, bias=False)\n",
              "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "            (softmax): Softmax(dim=-1)\n",
              "          )\n",
              "          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (drop_path1): DropPath(drop_prob=0.026)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (act): GELU(approximate='none')\n",
              "            (drop1): Dropout(p=0.0, inplace=False)\n",
              "            (norm): Identity()\n",
              "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (drop2): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (drop_path2): DropPath(drop_prob=0.026)\n",
              "        )\n",
              "        (3): SwinTransformerV2Block(\n",
              "          (attn): WindowAttention(\n",
              "            (cpb_mlp): Sequential(\n",
              "              (0): Linear(in_features=2, out_features=512, bias=True)\n",
              "              (1): ReLU(inplace=True)\n",
              "              (2): Linear(in_features=512, out_features=24, bias=False)\n",
              "            )\n",
              "            (qkv): Linear(in_features=768, out_features=2304, bias=False)\n",
              "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "            (softmax): Softmax(dim=-1)\n",
              "          )\n",
              "          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (drop_path1): DropPath(drop_prob=0.030)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (act): GELU(approximate='none')\n",
              "            (drop1): Dropout(p=0.0, inplace=False)\n",
              "            (norm): Identity()\n",
              "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (drop2): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (drop_path2): DropPath(drop_prob=0.030)\n",
              "        )\n",
              "        (4): SwinTransformerV2Block(\n",
              "          (attn): WindowAttention(\n",
              "            (cpb_mlp): Sequential(\n",
              "              (0): Linear(in_features=2, out_features=512, bias=True)\n",
              "              (1): ReLU(inplace=True)\n",
              "              (2): Linear(in_features=512, out_features=24, bias=False)\n",
              "            )\n",
              "            (qkv): Linear(in_features=768, out_features=2304, bias=False)\n",
              "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "            (softmax): Softmax(dim=-1)\n",
              "          )\n",
              "          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (drop_path1): DropPath(drop_prob=0.035)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (act): GELU(approximate='none')\n",
              "            (drop1): Dropout(p=0.0, inplace=False)\n",
              "            (norm): Identity()\n",
              "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (drop2): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (drop_path2): DropPath(drop_prob=0.035)\n",
              "        )\n",
              "        (5): SwinTransformerV2Block(\n",
              "          (attn): WindowAttention(\n",
              "            (cpb_mlp): Sequential(\n",
              "              (0): Linear(in_features=2, out_features=512, bias=True)\n",
              "              (1): ReLU(inplace=True)\n",
              "              (2): Linear(in_features=512, out_features=24, bias=False)\n",
              "            )\n",
              "            (qkv): Linear(in_features=768, out_features=2304, bias=False)\n",
              "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "            (softmax): Softmax(dim=-1)\n",
              "          )\n",
              "          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (drop_path1): DropPath(drop_prob=0.039)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (act): GELU(approximate='none')\n",
              "            (drop1): Dropout(p=0.0, inplace=False)\n",
              "            (norm): Identity()\n",
              "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (drop2): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (drop_path2): DropPath(drop_prob=0.039)\n",
              "        )\n",
              "        (6): SwinTransformerV2Block(\n",
              "          (attn): WindowAttention(\n",
              "            (cpb_mlp): Sequential(\n",
              "              (0): Linear(in_features=2, out_features=512, bias=True)\n",
              "              (1): ReLU(inplace=True)\n",
              "              (2): Linear(in_features=512, out_features=24, bias=False)\n",
              "            )\n",
              "            (qkv): Linear(in_features=768, out_features=2304, bias=False)\n",
              "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "            (softmax): Softmax(dim=-1)\n",
              "          )\n",
              "          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (drop_path1): DropPath(drop_prob=0.043)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (act): GELU(approximate='none')\n",
              "            (drop1): Dropout(p=0.0, inplace=False)\n",
              "            (norm): Identity()\n",
              "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (drop2): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (drop_path2): DropPath(drop_prob=0.043)\n",
              "        )\n",
              "        (7): SwinTransformerV2Block(\n",
              "          (attn): WindowAttention(\n",
              "            (cpb_mlp): Sequential(\n",
              "              (0): Linear(in_features=2, out_features=512, bias=True)\n",
              "              (1): ReLU(inplace=True)\n",
              "              (2): Linear(in_features=512, out_features=24, bias=False)\n",
              "            )\n",
              "            (qkv): Linear(in_features=768, out_features=2304, bias=False)\n",
              "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "            (softmax): Softmax(dim=-1)\n",
              "          )\n",
              "          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (drop_path1): DropPath(drop_prob=0.048)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (act): GELU(approximate='none')\n",
              "            (drop1): Dropout(p=0.0, inplace=False)\n",
              "            (norm): Identity()\n",
              "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (drop2): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (drop_path2): DropPath(drop_prob=0.048)\n",
              "        )\n",
              "        (8): SwinTransformerV2Block(\n",
              "          (attn): WindowAttention(\n",
              "            (cpb_mlp): Sequential(\n",
              "              (0): Linear(in_features=2, out_features=512, bias=True)\n",
              "              (1): ReLU(inplace=True)\n",
              "              (2): Linear(in_features=512, out_features=24, bias=False)\n",
              "            )\n",
              "            (qkv): Linear(in_features=768, out_features=2304, bias=False)\n",
              "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "            (softmax): Softmax(dim=-1)\n",
              "          )\n",
              "          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (drop_path1): DropPath(drop_prob=0.052)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (act): GELU(approximate='none')\n",
              "            (drop1): Dropout(p=0.0, inplace=False)\n",
              "            (norm): Identity()\n",
              "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (drop2): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (drop_path2): DropPath(drop_prob=0.052)\n",
              "        )\n",
              "        (9): SwinTransformerV2Block(\n",
              "          (attn): WindowAttention(\n",
              "            (cpb_mlp): Sequential(\n",
              "              (0): Linear(in_features=2, out_features=512, bias=True)\n",
              "              (1): ReLU(inplace=True)\n",
              "              (2): Linear(in_features=512, out_features=24, bias=False)\n",
              "            )\n",
              "            (qkv): Linear(in_features=768, out_features=2304, bias=False)\n",
              "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "            (softmax): Softmax(dim=-1)\n",
              "          )\n",
              "          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (drop_path1): DropPath(drop_prob=0.057)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (act): GELU(approximate='none')\n",
              "            (drop1): Dropout(p=0.0, inplace=False)\n",
              "            (norm): Identity()\n",
              "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (drop2): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (drop_path2): DropPath(drop_prob=0.057)\n",
              "        )\n",
              "        (10): SwinTransformerV2Block(\n",
              "          (attn): WindowAttention(\n",
              "            (cpb_mlp): Sequential(\n",
              "              (0): Linear(in_features=2, out_features=512, bias=True)\n",
              "              (1): ReLU(inplace=True)\n",
              "              (2): Linear(in_features=512, out_features=24, bias=False)\n",
              "            )\n",
              "            (qkv): Linear(in_features=768, out_features=2304, bias=False)\n",
              "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "            (softmax): Softmax(dim=-1)\n",
              "          )\n",
              "          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (drop_path1): DropPath(drop_prob=0.061)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (act): GELU(approximate='none')\n",
              "            (drop1): Dropout(p=0.0, inplace=False)\n",
              "            (norm): Identity()\n",
              "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (drop2): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (drop_path2): DropPath(drop_prob=0.061)\n",
              "        )\n",
              "        (11): SwinTransformerV2Block(\n",
              "          (attn): WindowAttention(\n",
              "            (cpb_mlp): Sequential(\n",
              "              (0): Linear(in_features=2, out_features=512, bias=True)\n",
              "              (1): ReLU(inplace=True)\n",
              "              (2): Linear(in_features=512, out_features=24, bias=False)\n",
              "            )\n",
              "            (qkv): Linear(in_features=768, out_features=2304, bias=False)\n",
              "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "            (softmax): Softmax(dim=-1)\n",
              "          )\n",
              "          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (drop_path1): DropPath(drop_prob=0.065)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (act): GELU(approximate='none')\n",
              "            (drop1): Dropout(p=0.0, inplace=False)\n",
              "            (norm): Identity()\n",
              "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (drop2): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (drop_path2): DropPath(drop_prob=0.065)\n",
              "        )\n",
              "        (12): SwinTransformerV2Block(\n",
              "          (attn): WindowAttention(\n",
              "            (cpb_mlp): Sequential(\n",
              "              (0): Linear(in_features=2, out_features=512, bias=True)\n",
              "              (1): ReLU(inplace=True)\n",
              "              (2): Linear(in_features=512, out_features=24, bias=False)\n",
              "            )\n",
              "            (qkv): Linear(in_features=768, out_features=2304, bias=False)\n",
              "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "            (softmax): Softmax(dim=-1)\n",
              "          )\n",
              "          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (drop_path1): DropPath(drop_prob=0.070)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (act): GELU(approximate='none')\n",
              "            (drop1): Dropout(p=0.0, inplace=False)\n",
              "            (norm): Identity()\n",
              "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (drop2): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (drop_path2): DropPath(drop_prob=0.070)\n",
              "        )\n",
              "        (13): SwinTransformerV2Block(\n",
              "          (attn): WindowAttention(\n",
              "            (cpb_mlp): Sequential(\n",
              "              (0): Linear(in_features=2, out_features=512, bias=True)\n",
              "              (1): ReLU(inplace=True)\n",
              "              (2): Linear(in_features=512, out_features=24, bias=False)\n",
              "            )\n",
              "            (qkv): Linear(in_features=768, out_features=2304, bias=False)\n",
              "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "            (softmax): Softmax(dim=-1)\n",
              "          )\n",
              "          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (drop_path1): DropPath(drop_prob=0.074)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (act): GELU(approximate='none')\n",
              "            (drop1): Dropout(p=0.0, inplace=False)\n",
              "            (norm): Identity()\n",
              "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (drop2): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (drop_path2): DropPath(drop_prob=0.074)\n",
              "        )\n",
              "        (14): SwinTransformerV2Block(\n",
              "          (attn): WindowAttention(\n",
              "            (cpb_mlp): Sequential(\n",
              "              (0): Linear(in_features=2, out_features=512, bias=True)\n",
              "              (1): ReLU(inplace=True)\n",
              "              (2): Linear(in_features=512, out_features=24, bias=False)\n",
              "            )\n",
              "            (qkv): Linear(in_features=768, out_features=2304, bias=False)\n",
              "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "            (softmax): Softmax(dim=-1)\n",
              "          )\n",
              "          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (drop_path1): DropPath(drop_prob=0.078)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (act): GELU(approximate='none')\n",
              "            (drop1): Dropout(p=0.0, inplace=False)\n",
              "            (norm): Identity()\n",
              "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (drop2): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (drop_path2): DropPath(drop_prob=0.078)\n",
              "        )\n",
              "        (15): SwinTransformerV2Block(\n",
              "          (attn): WindowAttention(\n",
              "            (cpb_mlp): Sequential(\n",
              "              (0): Linear(in_features=2, out_features=512, bias=True)\n",
              "              (1): ReLU(inplace=True)\n",
              "              (2): Linear(in_features=512, out_features=24, bias=False)\n",
              "            )\n",
              "            (qkv): Linear(in_features=768, out_features=2304, bias=False)\n",
              "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "            (softmax): Softmax(dim=-1)\n",
              "          )\n",
              "          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (drop_path1): DropPath(drop_prob=0.083)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (act): GELU(approximate='none')\n",
              "            (drop1): Dropout(p=0.0, inplace=False)\n",
              "            (norm): Identity()\n",
              "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (drop2): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (drop_path2): DropPath(drop_prob=0.083)\n",
              "        )\n",
              "        (16): SwinTransformerV2Block(\n",
              "          (attn): WindowAttention(\n",
              "            (cpb_mlp): Sequential(\n",
              "              (0): Linear(in_features=2, out_features=512, bias=True)\n",
              "              (1): ReLU(inplace=True)\n",
              "              (2): Linear(in_features=512, out_features=24, bias=False)\n",
              "            )\n",
              "            (qkv): Linear(in_features=768, out_features=2304, bias=False)\n",
              "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "            (softmax): Softmax(dim=-1)\n",
              "          )\n",
              "          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (drop_path1): DropPath(drop_prob=0.087)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (act): GELU(approximate='none')\n",
              "            (drop1): Dropout(p=0.0, inplace=False)\n",
              "            (norm): Identity()\n",
              "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (drop2): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (drop_path2): DropPath(drop_prob=0.087)\n",
              "        )\n",
              "        (17): SwinTransformerV2Block(\n",
              "          (attn): WindowAttention(\n",
              "            (cpb_mlp): Sequential(\n",
              "              (0): Linear(in_features=2, out_features=512, bias=True)\n",
              "              (1): ReLU(inplace=True)\n",
              "              (2): Linear(in_features=512, out_features=24, bias=False)\n",
              "            )\n",
              "            (qkv): Linear(in_features=768, out_features=2304, bias=False)\n",
              "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "            (softmax): Softmax(dim=-1)\n",
              "          )\n",
              "          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (drop_path1): DropPath(drop_prob=0.091)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (act): GELU(approximate='none')\n",
              "            (drop1): Dropout(p=0.0, inplace=False)\n",
              "            (norm): Identity()\n",
              "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (drop2): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (drop_path2): DropPath(drop_prob=0.091)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (3): SwinTransformerV2Stage(\n",
              "      (downsample): PatchMerging(\n",
              "        (reduction): Linear(in_features=3072, out_features=1536, bias=False)\n",
              "        (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
              "      )\n",
              "      (blocks): ModuleList(\n",
              "        (0): SwinTransformerV2Block(\n",
              "          (attn): WindowAttention(\n",
              "            (cpb_mlp): Sequential(\n",
              "              (0): Linear(in_features=2, out_features=512, bias=True)\n",
              "              (1): ReLU(inplace=True)\n",
              "              (2): Linear(in_features=512, out_features=48, bias=False)\n",
              "            )\n",
              "            (qkv): Linear(in_features=1536, out_features=4608, bias=False)\n",
              "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "            (proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
              "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "            (softmax): Softmax(dim=-1)\n",
              "          )\n",
              "          (norm1): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
              "          (drop_path1): DropPath(drop_prob=0.096)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=1536, out_features=6144, bias=True)\n",
              "            (act): GELU(approximate='none')\n",
              "            (drop1): Dropout(p=0.0, inplace=False)\n",
              "            (norm): Identity()\n",
              "            (fc2): Linear(in_features=6144, out_features=1536, bias=True)\n",
              "            (drop2): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (norm2): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
              "          (drop_path2): DropPath(drop_prob=0.096)\n",
              "        )\n",
              "        (1): SwinTransformerV2Block(\n",
              "          (attn): WindowAttention(\n",
              "            (cpb_mlp): Sequential(\n",
              "              (0): Linear(in_features=2, out_features=512, bias=True)\n",
              "              (1): ReLU(inplace=True)\n",
              "              (2): Linear(in_features=512, out_features=48, bias=False)\n",
              "            )\n",
              "            (qkv): Linear(in_features=1536, out_features=4608, bias=False)\n",
              "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "            (proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
              "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "            (softmax): Softmax(dim=-1)\n",
              "          )\n",
              "          (norm1): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
              "          (drop_path1): DropPath(drop_prob=0.100)\n",
              "          (mlp): Mlp(\n",
              "            (fc1): Linear(in_features=1536, out_features=6144, bias=True)\n",
              "            (act): GELU(approximate='none')\n",
              "            (drop1): Dropout(p=0.0, inplace=False)\n",
              "            (norm): Identity()\n",
              "            (fc2): Linear(in_features=6144, out_features=1536, bias=True)\n",
              "            (drop2): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (norm2): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
              "          (drop_path2): DropPath(drop_prob=0.100)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
              "  (head): ClassifierHead(\n",
              "    (global_pool): SelectAdaptivePool2d(pool_type=avg, flatten=Identity())\n",
              "    (drop): Dropout(p=0.0, inplace=False)\n",
              "    (fc): Linear(in_features=1536, out_features=25, bias=True)\n",
              "    (flatten): Identity()\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "cfg = {\n",
        "    'lr' : 5e-5,\n",
        "    'epochs' : 6,\n",
        "    'batch_size' : 8,\n",
        "    'is_low' : False,\n",
        "    'attempt_name' : '0416_swinv2_large_KD_student',\n",
        "    'method' : '고해상도 학습(Teacher, convNext_base) --> 저해상도 학습(student, swin_large_v2), KD',\n",
        "    'use_kfold' : False,\n",
        "    'use_distillation' : True,\n",
        "    'debug' : False\n",
        "}\n",
        "\n",
        "train_start(cfg, 1020, pretrained=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "pklzlsZq2K0O"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Byxv_Y2Qc9t5"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "6181dbc288654304a0886788e383821c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_993aaec9675d4bbfb1f836ee181f327c",
              "IPY_MODEL_8266fed6263241c2809a36988f1d1fb2",
              "IPY_MODEL_bcebbb006e4b41d49ebe072e191436ea"
            ],
            "layout": "IPY_MODEL_52aaccf97d9b42c4a9459203af40e7e8"
          }
        },
        "993aaec9675d4bbfb1f836ee181f327c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5f38eab31c61467db314c40e65f54443",
            "placeholder": "​",
            "style": "IPY_MODEL_68aadd0cb5354d818048a580351c3290",
            "value": "model.safetensors: 100%"
          }
        },
        "8266fed6263241c2809a36988f1d1fb2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a6a57bd386f74d8e801988787894aca2",
            "max": 792246652,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_84b71f4b35b4477199e3dc6a55be1f2f",
            "value": 792246652
          }
        },
        "bcebbb006e4b41d49ebe072e191436ea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6edbbce39fe245ce8e15cc7b0010e547",
            "placeholder": "​",
            "style": "IPY_MODEL_2a1ee68af53f4e348daa93c6d1605582",
            "value": " 792M/792M [00:09&lt;00:00, 73.0MB/s]"
          }
        },
        "52aaccf97d9b42c4a9459203af40e7e8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5f38eab31c61467db314c40e65f54443": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "68aadd0cb5354d818048a580351c3290": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a6a57bd386f74d8e801988787894aca2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "84b71f4b35b4477199e3dc6a55be1f2f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6edbbce39fe245ce8e15cc7b0010e547": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2a1ee68af53f4e348daa93c6d1605582": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}